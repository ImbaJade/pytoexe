# -*- coding: utf-8 -*-
"""LinearRegression23MarchAryan.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/15ThwnwoiCWjc680aVF8oNEBXxyzQzJlP

#**Linear Regression HandsOn**
Dataset link: https://drive.google.com/file/d/1kZ3BpQAdNtlEd-1dTpVyt_MgmXpNiyFe/view?usp=sharing

**import libraries**
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

"""**Read my data**"""

insurance = pd.read_csv('/content/new_insurance_data.csv')

insurance

insurance.head()

"""**Shape inspection**"""

insurance.shape

insurance.info()

# method chaining
insurance.isnull().sum()

insurance.isnull().sum().sum()

# Fill null values
insurance.columns

col = list(insurance.columns)
col

insurance['smoker'].mode()

insurance['smoker'].value_counts()

insurance['children'].value_counts()

col

for cname in col:    # Since we had children as categorical data
  if cname == 'children':
    insurance[cname] = insurance[cname].fillna(insurance[cname].mode()[0])
  elif insurance[cname].dtype =='object':
    insurance[cname] = insurance[cname].fillna(insurance[cname].mode()[0])
  else:
    insurance[cname] = insurance[cname].fillna(insurance[cname].mean())

# insurance['children'] = insurance['children'].fillna(insurance['children'].mode()[0])

# insurance['children'].mode()[0]

insurance.isnull().sum()

# We use outlier detecton only or numerical colums
for cname in col:
  if(insurance[cname].dtypes == 'int64' or insurance[cname].dtypes == 'float64'):
    plt.boxplot(insurance[cname])
    plt.xlabel(cname)
    plt.show()

# treat outliers
Q1 = insurance.bmi.quantile(0.25)
Q3 = insurance.bmi.quantile(0.75)
IQR = Q3 - Q1
# lower fence
lowerFence = Q1 - 1.5 * IQR
# upper Fence
upperFence = Q3 + 1.5*IQR
insurance = insurance[(insurance.bmi >= lowerFence) & (insurance.bmi <= upperFence)]
plt.boxplot(insurance.bmi)

# treat outliers
Q1 = insurance.past_consultations.quantile(0.25)
Q3 = insurance.past_consultations.quantile(0.75)
IQR = Q3 - Q1
# lower fence
lowerFence = Q1 - 1.5 * IQR
# upper Fence
upperFence = Q3 + 1.5*IQR
insurance = insurance[(insurance.past_consultations >= lowerFence) & (insurance.past_consultations <= upperFence)]
plt.boxplot(insurance.past_consultations)

# treat outliers
Q1 = insurance.Hospital_expenditure.quantile(0.25)
Q3 = insurance.Hospital_expenditure.quantile(0.75)
IQR = Q3 - Q1
# lower fence
lowerFence = Q1 - 1.5 * IQR
# upper Fence
upperFence = Q3 + 1.5*IQR
insurance = insurance[(insurance.Hospital_expenditure >= lowerFence) & (insurance.Hospital_expenditure <= upperFence)]
plt.boxplot(insurance.Hospital_expenditure)

# treat outliers
Q1 = insurance.Anual_Salary.quantile(0.25)
Q3 = insurance.Anual_Salary.quantile(0.75)
IQR = Q3 - Q1
# lower fence
lowerFence = Q1 - 1.5 * IQR
# upper Fence
upperFence = Q3 + 1.5*IQR
insurance = insurance[(insurance.Anual_Salary >= lowerFence) & (insurance.Anual_Salary <= upperFence)]
plt.boxplot(insurance.Anual_Salary)

"""**After removal of outliers once--- No need to do it again if some outliers are still there**"""

insurance.shape

# insurance ---- Corr()
numCols = insurance.select_dtypes(include = ['number'])
corrMat = numCols.corr()
corrMat

plt.figure(figsize = (10,5))
sns.heatmap(corrMat, annot = True, cmap = 'Greens')

"""# VIF ( variance Inflation Factor)
* Correlation  between two independent valus, features, columns are called **Multicolinearity**
---
**Note : VIF will tell me which independent columns are correlated**
* If VIF score is High --- Means highly correlated -> multicollinearity problem
* if VIF score is close below 5 --- Mostly variables are independent --- it is good for regression
---
* VIF = 1 --> no correlation (ideal)
* VIF < 5  ---> low correlation (okay)
* VIF >10 ---> High correlation (problem)

###**VIF works only for numerical colums**
"""

# python implementation
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)

col_list

X = insurance[col_list]

X

# X.values

# 1st try
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)
X = insurance[col_list]
vif_data = pd.DataFrame() # Empty datframe
vif_data['Features'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]
vif_data

insurance = insurance.drop(['num_of_steps'], axis = 1)

insurance.columns

# trail 2
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)
X = insurance[col_list]
vif_data = pd.DataFrame() # Empty datframe
vif_data['Features'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]
vif_data

insurance = insurance.drop(['age'], axis = 1)

# trail 3
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)
X = insurance[col_list]
vif_data = pd.DataFrame() # Empty datframe
vif_data['Features'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]
vif_data

insurance = insurance.drop(['bmi'], axis = 1)

# trail 4
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)
X = insurance[col_list]
vif_data = pd.DataFrame() # Empty datframe
vif_data['Features'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]
vif_data

insurance = insurance.drop(['NUmber_of_past_hospitalizations'], axis = 1)

# trail 5
from statsmodels.stats.outliers_influence import variance_inflation_factor
col_list = []
for col in insurance.columns:
  if((insurance[col].dtypes != 'object') & (col != 'charges')): # only num colms except target
    col_list.append(col)
X = insurance[col_list]
vif_data = pd.DataFrame() # Empty datframe
vif_data['Features'] = X.columns
vif_data['VIF'] = [variance_inflation_factor(X.values,i) for i in range(len(X.columns))]
vif_data

"""Now VIF score for other columns are less that 5 ---- Good for Regression model (Can now deal with overfitting)"""

insurance.columns

# Encoding for my categorical data (object types)
from sklearn.preprocessing import LabelEncoder
cat_cols = insurance.select_dtypes(include = ['object']).columns

le = LabelEncoder()
for col in cat_cols:
  insurance[col] = le.fit_transform(insurance[col])

insurance

"""# Splitting my data into train and test"""

insurance.columns

x = insurance.loc[:,['sex', 'children', 'smoker', 'Claim_Amount', 'past_consultations',
       'Hospital_expenditure', 'Anual_Salary', 'region']] # independent
y = insurance['charges'] # dependent

from sklearn.model_selection import train_test_split
x_train,x_test,y_train,y_test = train_test_split(x,y,train_size = 0.8,random_state = 0)

x_train # training question

y_train # training answers

x_test # Testing questions

y_test # testing answers (actual)

from sklearn.linear_model import LinearRegression

l_model = LinearRegression() # buildig the model

l_model.fit(x_train,y_train)

y_pred = l_model.predict(x_test)

y_pred

errors = pd.DataFrame(columns=['ActualData', 'PredictedData'])

errors['ActualData'] = y_test
errors['PredictedData'] = y_pred

errors

errors['Error'] = errors['ActualData'] - errors['PredictedData']

errors

from sklearn.metrics import *

RSquaredScore = r2_score(y_test,y_pred)

RSquaredScore # ---> 84%

sns.regplot(x=y_pred,y=y_test)
plt.xlabel('Predictions')
plt.ylabel('Actual values')
plt.show()

l_model.coef_ # m

l_model.intercept_ # c

# Fine Tuning (hyper parameters)
# Balancing Data -- Classification
# Standardization -- normalization
# Feature extraction ---
# Use mutiple model and chech which one is best